{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcYY2IvT4kq1",
        "outputId": "f8896216-dd6b-452d-9b2a-df819cc01193"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          Tumor Type        Location  Size (cm) Grade  Patient Age  Gender\n",
            "0  Oligodendroglioma  Occipital Lobe       9.23     I           48  Female\n",
            "1         Ependymoma  Occipital Lobe       0.87    II           47    Male\n",
            "2         Meningioma  Occipital Lobe       2.33    II           12  Female\n",
            "3         Ependymoma  Occipital Lobe       1.45   III           38  Female\n",
            "4         Ependymoma       Brainstem       6.45     I           35  Female\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/brain_tumor_dataset.csv')\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Encode categorical columns\n",
        "categorical_cols = ['Location', 'Grade', 'Gender']\n",
        "for col in categorical_cols:\n",
        "    df[col] = LabelEncoder().fit_transform(df[col])\n",
        "\n",
        "# Encode target column\n",
        "target_encoder = LabelEncoder()\n",
        "df['Tumor Type'] = target_encoder.fit_transform(df['Tumor Type'])  # Save this encoder if needed later\n",
        "\n",
        "# Define X and y\n",
        "X = df.drop(['Tumor Type'], axis=1)\n",
        "y = df['Tumor Type']\n"
      ],
      "metadata": {
        "id": "J1AyKSLU5V97"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "7f_YoYww5l-2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "99LmjV_T6YkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class ELM:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.input_weights = np.random.randn(self.input_size, self.hidden_size)\n",
        "        self.bias = np.random.randn(self.hidden_size)\n",
        "        self.output_weights = None\n",
        "        self.one_hot = False\n",
        "        self.classes = None\n",
        "\n",
        "    def _activation(self, x):\n",
        "        return 1 / (1 + np.exp(-x))  # Sigmoid activation\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # One-hot encode y if multi-class\n",
        "        if len(np.unique(y)) > 2:\n",
        "            self.one_hot = True\n",
        "            self.classes = np.unique(y)\n",
        "            y_onehot = np.zeros((y.shape[0], len(self.classes)))\n",
        "            for i, label in enumerate(y):\n",
        "                y_onehot[i, np.where(self.classes == label)[0][0]] = 1\n",
        "            y = y_onehot\n",
        "        else:\n",
        "            self.one_hot = False\n",
        "            y = np.array(y).reshape(-1, 1)  # FIXED HERE\n",
        "\n",
        "        H = self._activation(np.dot(X, self.input_weights) + self.bias)\n",
        "        self.output_weights = np.dot(np.linalg.pinv(H), y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        H = self._activation(np.dot(X, self.input_weights) + self.bias)\n",
        "        y_pred = np.dot(H, self.output_weights)\n",
        "        if self.one_hot:\n",
        "            return self.classes[np.argmax(y_pred, axis=1)]\n",
        "        else:\n",
        "            return (y_pred > 0.5).astype(int).flatten()\n",
        "\n",
        "# Initialize and train ELM\n",
        "elm = ELM(input_size=X_train.shape[1], hidden_size=100, output_size=1)\n",
        "elm.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred_elm = elm.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "elm_accuracy = accuracy_score(y_test, y_pred_elm)\n",
        "print(\"ELM Accuracy:\", elm_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JnZgrAXyODX",
        "outputId": "66369f8a-227d-4f77-9c28-bddad81abcf7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ELM Accuracy: 0.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "psUokCSK60vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Train decision tree classifier with default params\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "\n",
        "# 2. ELM (Baseline)\n",
        "elm = ELM(input_size=X_train.shape[1], hidden_size=100, output_size=1)\n",
        "elm.fit(X_train, y_train)\n",
        "y_pred_elm = elm.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Decision Tree Accuracy (Baseline):\", accuracy_score(y_test, y_pred_dt))\n",
        "print(\"ELM Accuracy (Baseline):\", accuracy_score(y_test, y_pred_elm))\n",
        "print(classification_report(y_test, y_pred_dt))\n",
        "print(classification_report(y_test, y_pred_elm))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b6ta5IIyUHO",
        "outputId": "7f190919-c5f8-466e-c312-2c0e3d8496c2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy (Baseline): 0.21\n",
            "ELM Accuracy (Baseline): 0.15\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.21      0.11      0.14        47\n",
            "           1       0.21      0.24      0.22        34\n",
            "           2       0.27      0.27      0.27        44\n",
            "           3       0.13      0.16      0.14        38\n",
            "           4       0.23      0.30      0.26        37\n",
            "\n",
            "    accuracy                           0.21       200\n",
            "   macro avg       0.21      0.21      0.21       200\n",
            "weighted avg       0.21      0.21      0.21       200\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.19      0.25        47\n",
            "           1       0.14      0.21      0.17        34\n",
            "           2       0.16      0.18      0.17        44\n",
            "           3       0.04      0.03      0.03        38\n",
            "           4       0.10      0.14      0.12        37\n",
            "\n",
            "    accuracy                           0.15       200\n",
            "   macro avg       0.16      0.15      0.15       200\n",
            "weighted avg       0.17      0.15      0.15       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "def dt_objective(params):\n",
        "    max_depth, min_samples_split = params\n",
        "    max_depth = int(max_depth)\n",
        "    min_samples_split = int(min_samples_split)\n",
        "\n",
        "    dt = DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split, random_state=42)\n",
        "    scores = cross_val_score(dt, X_train, y_train, cv=5, scoring='accuracy')\n",
        "    # Return negative accuracy as we minimize\n",
        "    return -scores.mean()\n"
      ],
      "metadata": {
        "id": "axuWp_TJyYSC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def fitness_function_hba(params, X_train, y_train, X_val, y_val):\n",
        "    max_depth = int(params[0])\n",
        "    min_samples_split = int(params[1])\n",
        "\n",
        "    if max_depth < 1: max_depth = 1\n",
        "    if min_samples_split < 2: min_samples_split = 2\n",
        "\n",
        "    clf = DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return 1 - accuracy_score(y_val, y_pred)  # Lower is better\n",
        "\n",
        "def honey_badger_algorithm(X_train, y_train, X_val, y_val, n_agents=10, max_iter=20):\n",
        "    dim = 2\n",
        "    lb = np.array([1, 2])\n",
        "    ub = np.array([20, 20])\n",
        "\n",
        "    # Initialize agents as float for operations\n",
        "    agents = np.random.randint(low=lb, high=ub + 1, size=(n_agents, dim)).astype(float)\n",
        "    best_fitness = float(\"inf\")\n",
        "    best_position = None\n",
        "\n",
        "    for iter in range(max_iter):\n",
        "        alpha = 2 * np.exp(-4 * (iter / max_iter) ** 2)\n",
        "        for i in range(n_agents):\n",
        "            fitness = fitness_function_hba(agents[i], X_train, y_train, X_val, y_val)\n",
        "            if fitness < best_fitness:\n",
        "                best_fitness = fitness\n",
        "                best_position = agents[i].copy()\n",
        "\n",
        "        for i in range(n_agents):\n",
        "            F = alpha * np.random.rand()\n",
        "            rand_agent = agents[np.random.randint(n_agents)]\n",
        "            if np.random.rand() < 0.5:\n",
        "                agents[i] += F * (rand_agent - agents[i])\n",
        "            else:\n",
        "                agents[i] -= F * (rand_agent - agents[i])\n",
        "            # Clamp within bounds\n",
        "            agents[i] = np.clip(agents[i], lb, ub)\n",
        "\n",
        "    return best_position.astype(int)\n",
        "\n",
        "# Train-validation split\n",
        "X_sub_train, X_val, y_sub_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Run HBA optimizer\n",
        "best_params_hba = honey_badger_algorithm(X_sub_train, y_sub_train, X_val, y_val)\n",
        "print(\"Best HBA Parameters (max_depth, min_samples_split):\", best_params_hba)\n",
        "\n",
        "# Train final Decision Tree using best parameters\n",
        "clf_hba = DecisionTreeClassifier(max_depth=best_params_hba[0], min_samples_split=best_params_hba[1])\n",
        "clf_hba.fit(X_train, y_train)\n",
        "y_pred_hba = clf_hba.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "hba_accuracy = accuracy_score(y_test, y_pred_hba)\n",
        "print(\"HBA-Optimized Decision Tree Accuracy:\", hba_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiNeRuaXzP0F",
        "outputId": "51dae96a-e96d-48e2-8a9b-8412a6be131a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best HBA Parameters (max_depth, min_samples_split): [ 9 11]\n",
            "HBA-Optimized Decision Tree Accuracy: 0.165\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_max_depth = int(best_params_hba[0])\n",
        "best_min_samples_split = int(best_params_hba[1])\n",
        "\n",
        "dt_hba = DecisionTreeClassifier(max_depth=best_max_depth, min_samples_split=best_min_samples_split, random_state=42)\n",
        "dt_hba.fit(X_train, y_train)\n",
        "\n",
        "y_pred_hba = dt_hba.predict(X_test)\n",
        "\n",
        "print(\"Optimized Decision Tree Accuracy (HBA):\", accuracy_score(y_test, y_pred_hba))\n",
        "print(classification_report(y_test, y_pred_hba))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jatbyqyezY0j",
        "outputId": "72991771-17c6-4e5a-9e23-acef7640378e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized Decision Tree Accuracy (HBA): 0.165\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        47\n",
            "           1       0.16      0.82      0.27        34\n",
            "           2       0.20      0.05      0.07        44\n",
            "           3       0.00      0.00      0.00        38\n",
            "           4       0.33      0.08      0.13        37\n",
            "\n",
            "    accuracy                           0.17       200\n",
            "   macro avg       0.14      0.19      0.10       200\n",
            "weighted avg       0.13      0.17      0.09       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "def dt_objective(params):\n",
        "    max_depth, min_samples_split = params\n",
        "    max_depth = int(max_depth)\n",
        "    min_samples_split = int(min_samples_split)\n",
        "\n",
        "    dt = DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split, random_state=42)\n",
        "    scores = cross_val_score(dt, X_train, y_train, cv=5, scoring='accuracy')\n",
        "    # Return negative accuracy as we minimize\n",
        "    return -scores.mean()\n"
      ],
      "metadata": {
        "id": "XSm5mzuIE4k0"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def fitness_function_hba(params, X_train, y_train, X_val, y_val):\n",
        "    max_depth = int(params[0])\n",
        "    min_samples_split = int(params[1])\n",
        "\n",
        "    if max_depth < 1: max_depth = 1\n",
        "    if min_samples_split < 2: min_samples_split = 2\n",
        "\n",
        "    clf = DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return 1 - accuracy_score(y_val, y_pred)  # Lower is better\n",
        "\n",
        "def honey_badger_algorithm(X_train, y_train, X_val, y_val, n_agents=10, max_iter=20):\n",
        "    dim = 2\n",
        "    lb = np.array([1, 2])\n",
        "    ub = np.array([20, 20])\n",
        "\n",
        "    # Initialize agents as float for operations\n",
        "    agents = np.random.randint(low=lb, high=ub + 1, size=(n_agents, dim)).astype(float)\n",
        "    best_fitness = float(\"inf\")\n",
        "    best_position = None\n",
        "\n",
        "    for iter in range(max_iter):\n",
        "        alpha = 2 * np.exp(-4 * (iter / max_iter) ** 2)\n",
        "        for i in range(n_agents):\n",
        "            fitness = fitness_function_hba(agents[i], X_train, y_train, X_val, y_val)\n",
        "            if fitness < best_fitness:\n",
        "                best_fitness = fitness\n",
        "                best_position = agents[i].copy()\n",
        "\n",
        "        for i in range(n_agents):\n",
        "            F = alpha * np.random.rand()\n",
        "            rand_agent = agents[np.random.randint(n_agents)]\n",
        "            if np.random.rand() < 0.5:\n",
        "                agents[i] += F * (rand_agent - agents[i])\n",
        "            else:\n",
        "                agents[i] -= F * (rand_agent - agents[i])\n",
        "            # Clamp within bounds\n",
        "            agents[i] = np.clip(agents[i], lb, ub)\n",
        "\n",
        "    return best_position.astype(int)\n",
        "\n",
        "# Train-validation split\n",
        "X_sub_train, X_val, y_sub_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Run HBA optimizer\n",
        "best_params_hba = honey_badger_algorithm(X_sub_train, y_sub_train, X_val, y_val)\n",
        "print(\"Best HBA Parameters (max_depth, min_samples_split):\", best_params_hba)\n",
        "\n",
        "# Train final Decision Tree using best parameters\n",
        "clf_hba = DecisionTreeClassifier(max_depth=best_params_hba[0], min_samples_split=best_params_hba[1])\n",
        "clf_hba.fit(X_train, y_train)\n",
        "y_pred_hba = clf_hba.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "hba_accuracy = accuracy_score(y_test, y_pred_hba)\n",
        "print(\"HBA-Optimized Decision Tree Accuracy:\", hba_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d159270d-40ff-414e-ca7e-1e1a18cf506a",
        "id": "Qo-ETCAlFJQr"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best HBA Parameters (max_depth, min_samples_split): [12  4]\n",
            "HBA-Optimized Decision Tree Accuracy: 0.195\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_max_depth = int(best_params_hba[0])\n",
        "best_min_samples_split = int(best_params_hba[1])\n",
        "\n",
        "dt_hba = DecisionTreeClassifier(max_depth=best_max_depth, min_samples_split=best_min_samples_split, random_state=42)\n",
        "dt_hba.fit(X_train, y_train)\n",
        "\n",
        "y_pred_hba = dt_hba.predict(X_test)\n",
        "\n",
        "print(\"Optimized Decision Tree Accuracy (HBA):\", accuracy_score(y_test, y_pred_hba))\n",
        "print(classification_report(y_test, y_pred_hba))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e15f2aa-889b-4f0f-a913-d47713d209dc",
        "id": "RvnV9o6dFJQs"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized Decision Tree Accuracy (HBA): 0.195\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        47\n",
            "           1       0.18      0.68      0.29        34\n",
            "           2       0.25      0.32      0.28        44\n",
            "           3       0.00      0.00      0.00        38\n",
            "           4       0.29      0.05      0.09        37\n",
            "\n",
            "    accuracy                           0.20       200\n",
            "   macro avg       0.14      0.21      0.13       200\n",
            "weighted avg       0.14      0.20      0.13       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def fitness_function_ssa(params, X_train, y_train, X_val, y_val):\n",
        "    max_depth = int(params[0])\n",
        "    min_samples_split = int(params[1])\n",
        "\n",
        "    if max_depth < 1: max_depth = 1\n",
        "    if min_samples_split < 2: min_samples_split = 2\n",
        "\n",
        "    clf = DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return 1 - accuracy_score(y_val, y_pred)  # Lower is better\n",
        "\n",
        "def salp_swarm_algorithm(X_train, y_train, X_val, y_val, n_salps=10, max_iter=20):\n",
        "    dim = 2\n",
        "    lb = np.array([1, 2])\n",
        "    ub = np.array([20, 20])\n",
        "\n",
        "    # Initialize salps as float\n",
        "    salps = np.random.randint(lb[0], ub[0] + 1, size=(n_salps, dim)).astype(float)\n",
        "    food_position = None\n",
        "    food_fitness = float(\"inf\")\n",
        "\n",
        "    for iter in range(max_iter):\n",
        "        c1 = 2 * np.exp(-(4 * iter / max_iter) ** 2)  # convergence factor\n",
        "\n",
        "        for i in range(n_salps):\n",
        "            fitness = fitness_function_ssa(salps[i], X_train, y_train, X_val, y_val)\n",
        "            if fitness < food_fitness:\n",
        "                food_fitness = fitness\n",
        "                food_position = salps[i].copy()\n",
        "\n",
        "        for i in range(n_salps):\n",
        "            for j in range(dim):\n",
        "                if i == 0:  # leader\n",
        "                    c2 = np.random.rand()\n",
        "                    c3 = np.random.rand()\n",
        "                    if c3 < 0.5:\n",
        "                        salps[i, j] = food_position[j] + c1 * ((ub[j] - lb[j]) * c2 + lb[j])\n",
        "                    else:\n",
        "                        salps[i, j] = food_position[j] - c1 * ((ub[j] - lb[j]) * c2 + lb[j])\n",
        "                else:  # followers\n",
        "                    salps[i, j] = (salps[i, j] + salps[i - 1, j]) / 2.0\n",
        "\n",
        "            # Clamp to bounds\n",
        "            salps[i] = np.clip(salps[i], lb, ub)\n",
        "\n",
        "    return food_position.astype(int)\n",
        "\n",
        "# Train-validation split\n",
        "X_sub_train, X_val, y_sub_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Run SSA optimizer\n",
        "best_params_ssa = salp_swarm_algorithm(X_sub_train, y_sub_train, X_val, y_val)\n",
        "print(\"Best SSA Parameters (max_depth, min_samples_split):\", best_params_ssa)\n",
        "\n",
        "# Train final Decision Tree using best SSA parameters\n",
        "clf_ssa = DecisionTreeClassifier(max_depth=best_params_ssa[0], min_samples_split=best_params_ssa[1])\n",
        "clf_ssa.fit(X_train, y_train)\n",
        "y_pred_ssa = clf_ssa.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "ssa_accuracy = accuracy_score(y_test, y_pred_ssa)\n",
        "print(\"SSA-Optimized Decision Tree Accuracy:\", ssa_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUjdbeNuzzL7",
        "outputId": "b608d67c-eadf-4e6e-9bf1-8051da08457e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best SSA Parameters (max_depth, min_samples_split): [14  4]\n",
            "SSA-Optimized Decision Tree Accuracy: 0.19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fitness_function_elm(hidden_size, X_train, y_train, X_val, y_val):\n",
        "    hidden_size = int(hidden_size[0])\n",
        "    hidden_size = max(1, min(hidden_size, 500))\n",
        "    elm = ELM(input_size=X_train.shape[1], hidden_size=hidden_size, output_size=1)\n",
        "    elm.fit(X_train, y_train)\n",
        "    y_pred = elm.predict(X_val)\n",
        "    return 1 - accuracy_score(y_val, y_pred)\n",
        "\n",
        "def honey_badger_elm(X_train, y_train, X_val, y_val, n_agents=10, max_iter=20):\n",
        "    lb = np.array([10])\n",
        "    ub = np.array([500])\n",
        "    agents = np.random.randint(low=lb, high=ub + 1, size=(n_agents, 1)).astype(float)\n",
        "    best_fitness = float(\"inf\")\n",
        "    best_position = None\n",
        "    for iter in range(max_iter):\n",
        "        alpha = 2 * np.exp(-4 * (iter / max_iter) ** 2)\n",
        "        for i in range(n_agents):\n",
        "            fitness = fitness_function_elm(agents[i], X_train, y_train, X_val, y_val)\n",
        "            if fitness < best_fitness:\n",
        "                best_fitness = fitness\n",
        "                best_position = agents[i].copy()\n",
        "        for i in range(n_agents):\n",
        "            F = alpha * np.random.rand()\n",
        "            rand_agent = agents[np.random.randint(n_agents)]\n",
        "            if np.random.rand() < 0.5:\n",
        "                agents[i] += F * (rand_agent - agents[i])\n",
        "            else:\n",
        "                agents[i] -= F * (rand_agent - agents[i])\n",
        "            agents[i] = np.clip(agents[i], lb, ub)\n",
        "    return int(best_position[0])\n",
        "\n",
        "    # ELM-HBA\n",
        "best_hidden_hba = honey_badger_elm(X_sub_train, y_sub_train, X_val, y_val)\n",
        "elm_hba = ELM(input_size=X_train.shape[1], hidden_size=best_hidden_hba, output_size=1)\n",
        "elm_hba.fit(X_train, y_train)\n",
        "y_pred_elm_hba = elm_hba.predict(X_test)\n",
        "elm_hba_acc = accuracy_score(y_test, y_pred_elm_hba)\n",
        "print(\"Optimized ELM  (HBA):\", accuracy_score(y_test, y_pred_elm_hba))\n",
        "print(classification_report(y_test, y_pred_elm_hba))\n",
        "\n",
        "def salp_swarm_elm(X_train, y_train, X_val, y_val, n_salps=10, max_iter=20):\n",
        "    lb = np.array([10])\n",
        "    ub = np.array([500])\n",
        "    salps = np.random.randint(lb[0], ub[0] + 1, size=(n_salps, 1)).astype(float)\n",
        "    food_position = None\n",
        "    food_fitness = float(\"inf\")\n",
        "    for iter in range(max_iter):\n",
        "        c1 = 2 * np.exp(-(4 * iter / max_iter) ** 2)\n",
        "        for i in range(n_salps):\n",
        "            fitness = fitness_function_elm(salps[i], X_train, y_train, X_val, y_val)\n",
        "            if fitness < food_fitness:\n",
        "                food_fitness = fitness\n",
        "                food_position = salps[i].copy()\n",
        "        for i in range(n_salps):\n",
        "            for j in range(1):\n",
        "                if i == 0:\n",
        "                    c2 = np.random.rand()\n",
        "                    c3 = np.random.rand()\n",
        "                    if c3 < 0.5:\n",
        "                        salps[i, j] = food_position[j] + c1 * ((ub[j] - lb[j]) * c2 + lb[j])\n",
        "                    else:\n",
        "                        salps[i, j] = food_position[j] - c1 * ((ub[j] - lb[j]) * c2 + lb[j])\n",
        "                else:\n",
        "                    salps[i, j] = (salps[i, j] + salps[i - 1, j]) / 2.0\n",
        "            salps[i] = np.clip(salps[i], lb, ub)\n",
        "    return int(food_position[0])\n",
        "\n",
        "    # ELM-SSA\n",
        "best_hidden_ssa = salp_swarm_elm(X_sub_train, y_sub_train, X_val, y_val)\n",
        "elm_ssa = ELM(input_size=X_train.shape[1], hidden_size=best_hidden_ssa, output_size=1)\n",
        "elm_ssa.fit(X_train, y_train)\n",
        "y_pred_elm_ssa = elm_ssa.predict(X_test)\n",
        "elm_ssa_acc = accuracy_score(y_test, y_pred_elm_ssa)\n",
        "print(\"Optimized ELM (SSA):\", accuracy_score(y_test, y_pred_elm_ssa))\n",
        "print(classification_report(y_test, y_pred_elm_ssa))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsCp3BMGRBjT",
        "outputId": "2b552c0c-b598-4d4a-d4cd-43cbc5502318"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized ELM  (HBA): 0.195\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.24      0.21      0.23        47\n",
            "           1       0.11      0.12      0.11        34\n",
            "           2       0.20      0.20      0.20        44\n",
            "           3       0.26      0.21      0.23        38\n",
            "           4       0.17      0.22      0.19        37\n",
            "\n",
            "    accuracy                           0.20       200\n",
            "   macro avg       0.20      0.19      0.19       200\n",
            "weighted avg       0.20      0.20      0.20       200\n",
            "\n",
            "Optimized ELM (SSA): 0.15\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.12      0.11      0.11        47\n",
            "           1       0.07      0.09      0.08        34\n",
            "           2       0.23      0.20      0.22        44\n",
            "           3       0.19      0.16      0.17        38\n",
            "           4       0.15      0.19      0.17        37\n",
            "\n",
            "    accuracy                           0.15       200\n",
            "   macro avg       0.15      0.15      0.15       200\n",
            "weighted avg       0.16      0.15      0.15       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy Summary\n",
        "print(\"\\n====== Final Accuracy Summary ======\")\n",
        "print(f\"1. Decision Tree (Baseline):         {accuracy_score(y_test, y_pred_dt):.4f}\")\n",
        "print(f\"2. ELM (Baseline):                   {elm_accuracy:.4f}\")\n",
        "print(f\"3. Decision Tree (HBA):              {hba_accuracy:.4f}\")\n",
        "print(f\"4. Decision Tree (SSA):              {ssa_accuracy:.4f}\")\n",
        "print(f\"5. ELM Optimized (HBA):              {elm_hba_acc:.4f}\")\n",
        "print(f\"6. ELM Optimized (SSA):              {elm_ssa_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqORHWsBz3Hi",
        "outputId": "6be7224c-c136-467a-a34f-08156bc3a7e9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====== Final Accuracy Summary ======\n",
            "1. Decision Tree (Baseline):         0.2100\n",
            "2. ELM (Baseline):                   0.2100\n",
            "3. Decision Tree (HBA):              0.1950\n",
            "4. Decision Tree (SSA):              0.1900\n",
            "5. ELM Optimized (HBA):              0.1950\n",
            "6. ELM Optimized (SSA):              0.1500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "def evaluate_model(name, y_true, y_pred):\n",
        "    print(f\"\\n==== {name} ====\")\n",
        "    print(classification_report(y_true, y_pred))\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "    if cm.shape == (2, 2):  # Binary classification\n",
        "        TN, FP, FN, TP = cm.ravel()\n",
        "        sensitivity = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
        "        specificity = TN / (TN + FP) if (TN + FP) != 0 else 0\n",
        "\n",
        "        print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
        "        print(f\"Specificity: {specificity:.4f}\")\n",
        "    else:\n",
        "        print(\"Multi-class detected: Sensitivity/Specificity not computed.\")\n",
        "\n",
        "# Apply to all models\n",
        "evaluate_model(\"1. Decision Tree (Baseline)\", y_test, y_pred_dt)\n",
        "evaluate_model(\"2. ELM (Baseline)\", y_test, y_pred_elm)\n",
        "evaluate_model(\"3. Decision Tree (HBA)\", y_test, y_pred_hba)\n",
        "evaluate_model(\"4. Decision Tree (SSA)\", y_test, y_pred_ssa)\n",
        "evaluate_model(\"5. ELM (HBA)\", y_test, y_pred_elm_hba)\n",
        "evaluate_model(\"6. ELM (SSA)\", y_test, y_pred_elm_ssa)\n"
      ],
      "metadata": {
        "id": "nxFGy-ElcUU5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99e34572-ebd7-4fcf-d67d-6513a2671c26"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== 1. Decision Tree (Baseline) ====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.21      0.11      0.14        47\n",
            "           1       0.21      0.24      0.22        34\n",
            "           2       0.27      0.27      0.27        44\n",
            "           3       0.13      0.16      0.14        38\n",
            "           4       0.23      0.30      0.26        37\n",
            "\n",
            "    accuracy                           0.21       200\n",
            "   macro avg       0.21      0.21      0.21       200\n",
            "weighted avg       0.21      0.21      0.21       200\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 5 13  9 11  9]\n",
            " [ 3  8 11  6  6]\n",
            " [ 7  7 12 12  6]\n",
            " [ 3  6  8  6 15]\n",
            " [ 6  5  5 10 11]]\n",
            "Multi-class detected: Sensitivity/Specificity not computed.\n",
            "\n",
            "==== 2. ELM (Baseline) ====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.19      0.25        47\n",
            "           1       0.14      0.21      0.17        34\n",
            "           2       0.16      0.18      0.17        44\n",
            "           3       0.04      0.03      0.03        38\n",
            "           4       0.10      0.14      0.12        37\n",
            "\n",
            "    accuracy                           0.15       200\n",
            "   macro avg       0.16      0.15      0.15       200\n",
            "weighted avg       0.17      0.15      0.15       200\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 9 12  8 10  8]\n",
            " [ 2  7  8  5 12]\n",
            " [ 4 12  8  7 13]\n",
            " [ 5  9 13  1 10]\n",
            " [ 4 10 14  4  5]]\n",
            "Multi-class detected: Sensitivity/Specificity not computed.\n",
            "\n",
            "==== 3. Decision Tree (HBA) ====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        47\n",
            "           1       0.18      0.68      0.29        34\n",
            "           2       0.25      0.32      0.28        44\n",
            "           3       0.00      0.00      0.00        38\n",
            "           4       0.29      0.05      0.09        37\n",
            "\n",
            "    accuracy                           0.20       200\n",
            "   macro avg       0.14      0.21      0.13       200\n",
            "weighted avg       0.14      0.20      0.13       200\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 0 30 13  2  2]\n",
            " [ 1 23  8  1  1]\n",
            " [ 1 28 14  1  0]\n",
            " [ 0 25 11  0  2]\n",
            " [ 0 20 11  4  2]]\n",
            "Multi-class detected: Sensitivity/Specificity not computed.\n",
            "\n",
            "==== 4. Decision Tree (SSA) ====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        47\n",
            "           1       0.17      0.56      0.26        34\n",
            "           2       0.25      0.32      0.28        44\n",
            "           3       0.00      0.00      0.00        38\n",
            "           4       0.25      0.14      0.18        37\n",
            "\n",
            "    accuracy                           0.19       200\n",
            "   macro avg       0.13      0.20      0.14       200\n",
            "weighted avg       0.13      0.19      0.14       200\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 0 28 14  3  2]\n",
            " [ 1 19  8  1  5]\n",
            " [ 1 26 14  1  2]\n",
            " [ 0 21 11  0  6]\n",
            " [ 1 17 10  4  5]]\n",
            "Multi-class detected: Sensitivity/Specificity not computed.\n",
            "\n",
            "==== 5. ELM (HBA) ====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.24      0.21      0.23        47\n",
            "           1       0.11      0.12      0.11        34\n",
            "           2       0.20      0.20      0.20        44\n",
            "           3       0.26      0.21      0.23        38\n",
            "           4       0.17      0.22      0.19        37\n",
            "\n",
            "    accuracy                           0.20       200\n",
            "   macro avg       0.20      0.19      0.19       200\n",
            "weighted avg       0.20      0.20      0.20       200\n",
            "\n",
            "Confusion Matrix:\n",
            " [[10  8 10  4 15]\n",
            " [ 8  4  8  4 10]\n",
            " [ 9  6  9 11  9]\n",
            " [ 7  6 11  8  6]\n",
            " [ 7 12  6  4  8]]\n",
            "Multi-class detected: Sensitivity/Specificity not computed.\n",
            "\n",
            "==== 6. ELM (SSA) ====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.12      0.11      0.11        47\n",
            "           1       0.07      0.09      0.08        34\n",
            "           2       0.23      0.20      0.22        44\n",
            "           3       0.19      0.16      0.17        38\n",
            "           4       0.15      0.19      0.17        37\n",
            "\n",
            "    accuracy                           0.15       200\n",
            "   macro avg       0.15      0.15      0.15       200\n",
            "weighted avg       0.16      0.15      0.15       200\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 5  9 10  9 14]\n",
            " [12  3  4  6  9]\n",
            " [ 7 12  9  6 10]\n",
            " [11  6  9  6  6]\n",
            " [ 6 12  7  5  7]]\n",
            "Multi-class detected: Sensitivity/Specificity not computed.\n"
          ]
        }
      ]
    }
  ]
}